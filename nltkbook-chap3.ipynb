{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 3: Processing Raw Text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9c2eb58aa5243b6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from urllib import request"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:37.832149Z",
     "start_time": "2024-04-04T10:08:37.822762Z"
    }
   },
   "id": "2e1af2ea9c073a19",
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 1176812 ï»¿The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\r\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.gutenberg.org/files/2554/2554-0.txt'\n",
    "response = request.urlopen(url) \n",
    "raw = response.read().decode('utf-8')\n",
    "print(type(raw), len(raw), raw[:75])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:40.138852Z",
     "start_time": "2024-04-04T10:08:37.910983Z"
    }
   },
   "id": "417a11f454819a90",
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 257058 ['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "print(type(tokens), len(tokens), tokens[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:40.849041Z",
     "start_time": "2024-04-04T10:08:40.143024Z"
    }
   },
   "id": "e33f436b06458252",
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['insight', 'impresses', 'us', 'as', 'wisdom', '...', 'that', 'wisdom', 'of', 'the', 'heart', 'which', 'we', 'seek', 'that', 'we', 'may', 'learn', 'from', 'it', 'how', 'to', 'live', '.', 'All', 'his', 'other', 'gifts', 'came', 'to', 'him', 'from', 'nature', ',', 'this', 'he', 'won', 'for']\n",
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Project Gutenberg; Ilya\n",
      "Petrovitch; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(text[1024:1062])\n",
    "print(text.collocations())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:41.124525Z",
     "start_time": "2024-04-04T10:08:40.849773Z"
    }
   },
   "id": "60f3508389663891",
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 37306\n",
      "<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\n"
     ]
    }
   ],
   "source": [
    "url1 = 'http://news.bbc.co.uk/2/hi/health/2284783.stm'\n",
    "html = request.urlopen(url1).read().decode('utf8')\n",
    "print(type(html), len(html))\n",
    "print(html[:60])\n",
    "#print(html)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:41.663986Z",
     "start_time": "2024-04-04T10:08:41.125850Z"
    }
   },
   "id": "4cb667ae56daf57f",
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 7 of 7 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n",
      "er's Polio campaign launched in Iraq Gene defect explains high blood pressure \n",
      "er's Polio campaign launched in Iraq Gene defect explains high blood pressure \n",
      "24338\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "raw1 = BeautifulSoup(html, 'html.parser').get_text() \n",
    "tokens_html = word_tokenize(raw1)\n",
    "text_html = nltk.Text(tokens_html)\n",
    "text_html.concordance('gene')\n",
    "#print(tokens_html)\n",
    "\n",
    "print(html.find('of the'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:41.701814Z",
     "start_time": "2024-04-04T10:08:41.666367Z"
    }
   },
   "id": "8583cd034c0699ad",
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hp_1 = open('/Users/maria.onoeva/Desktop/new_folder/HP_1.txt')\n",
    "raw_HP = hp_1.read()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:41.708036Z",
     "start_time": "2024-04-04T10:08:41.702961Z"
    }
   },
   "id": "e47be2abe3215826",
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            very\n",
      "          veryvery\n",
      "        veryveryvery\n",
      "      veryveryveryvery\n",
      "    veryveryveryveryvery\n",
      "  veryveryveryveryveryvery\n",
      "veryveryveryveryveryveryvery\n",
      "  veryveryveryveryveryvery\n",
      "    veryveryveryveryvery\n",
      "      veryveryveryvery\n",
      "        veryveryvery\n",
      "          veryvery\n",
      "            very\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\n",
    "b = [' ' * 2 * (7 - i) + 'very' * i for i in a]\n",
    "for line in b:\n",
    "    print(line)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:41.712492Z",
     "start_time": "2024-04-04T10:08:41.708993Z"
    }
   },
   "id": "9f94c64b7773b66",
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's\n",
      "the\n",
      "worst\n",
      "case\n",
      "scenario\n",
      "lullaby\n",
      "I t ' s   t h e   w o r s t   c a s e   s c e n a r i o   l u l l a b y "
     ]
    }
   ],
   "source": [
    "my_sentence = 'It\\'s the worst case scenario lullaby'\n",
    "for i in my_sentence.split(' '):\n",
    "    print(i)\n",
    "    \n",
    "for char in my_sentence: \n",
    "    print(char, end=' ')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:41.716526Z",
     "start_time": "2024-04-04T10:08:41.713753Z"
    }
   },
   "id": "7b8288161b316ba1",
   "execution_count": 123
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'aa', 'aba', 'abac', 'abaca', 'abaff', 'abb', 'abed', 'acca', 'accede', 'ace', 'ad', 'adad', 'add', 'adda', 'added', 'ade', 'adead', 'ae', 'aface', 'affa', 'ajaja', 'b', 'ba', 'baa', 'baba', 'babe', 'bac', 'bacaba', 'bacca', 'baccae', 'bad', 'bade', 'bae', 'baff', 'bajada', 'be', 'bead', 'beaded', 'bebed', 'bed', 'bedad', 'bedded', 'bedead', 'bedeaf', 'bee', 'beef', 'bejade', 'c', 'ca', 'cab', 'caba', 'cabda', 'cad', 'cade', 'caeca', 'caffa', 'ce', 'cede', 'cee', 'd', 'da', 'dab', 'dabb', 'dabba', 'dace', 'dad', 'dada', 'dade', 'dae', 'daff', 'de', 'dead', 'deaf', 'deb', 'decad', 'decade', 'dee', 'deed', 'deedeed', 'deface', 'e', 'ea', 'ebb', 'ecad', 'edea', 'efface', 'f', 'fa', 'facade', 'face', 'faced', 'fad', 'fade', 'faded', 'fae', 'faff', 'fe', 'fed', 'fee', 'feed', 'j', 'jab', 'jabbed', 'jade', 'jaded', 'jed', 'jeff']\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'j', 'k', 'l', 'm', 'n', 'o']\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o']\n"
     ]
    }
   ],
   "source": [
    "word_list = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "# print([w for w in word_list if re.search('ed$', w)])\n",
    "# print([w for w in word_list if re.search('^..j..t..$', w)])\n",
    "# print([w for w in word_list if re.search('..j..t..', w)])\n",
    "# print([w for w in word_list if re.search('^[g-o]+$', w)])\n",
    "# print([w for w in word_list if re.search('^[g-o]$', w)])\n",
    "print([w for w in word_list if re.search('^[a-fj]+$', w)])\n",
    "print([w for w in word_list if re.search('^[a-fj-o]$', w)])\n",
    "print([w for w in word_list if re.search('^[a-o]$', w)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:41.999902Z",
     "start_time": "2024-04-04T10:08:41.717374Z"
    }
   },
   "id": "9403ab0726ddab0a",
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'objectee', 'objector', 'rejecter', 'rejector', 'unjilted', 'unjolted', 'unjustly']\n",
      "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'objectee', 'objector', 'rejecter', 'rejector', 'unjilted', 'unjolted', 'unjustly']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in word_list if re.search('^..j..t..$', w)])\n",
    "print([w for w in word_list if re.search(r'^..j..t..$', w)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.133773Z",
     "start_time": "2024-04-04T10:08:42.002038Z"
    }
   },
   "id": "6003f991e5d91e7d",
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "\\b\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/26318287/what-does-r-mean-before-a-regex-pattern\n",
    "print('\\n') # Prints a newline character\n",
    "print(r'\\n') # Escape sequence is not processed\n",
    "print('\\b') # Prints a backspace character\n",
    "print(r'\\b') # Escape sequence is not processed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.136392Z",
     "start_time": "2024-04-04T10:08:42.134495Z"
    }
   },
   "id": "63eb5733d21fbc30",
   "execution_count": 126
  },
  {
   "cell_type": "markdown",
   "source": [
    "I guess I got it. So by prefixing with 'r' we kinda say to Python not to read into that but simply to pass a combination to re library, ok!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6b535d84843aa1b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2009, 12, 31]\n"
     ]
    }
   ],
   "source": [
    "print([int(n) for n in re.findall(r'[0-9]{2,}', '2009-12-31')])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.140910Z",
     "start_time": "2024-04-04T10:08:42.137025Z"
    }
   },
   "id": "f902aa2546d6c412",
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{P}lease could you stop the noise?\n",
      "{I}'m tryna get some rest\n",
      "{F}rom all the unborn chicken\n",
      "{V}oices in my head\n"
     ]
    }
   ],
   "source": [
    "paranoid = \"\"\"Please could you stop the noise?\n",
    "I'm tryna get some rest\n",
    "From all the unborn chicken\n",
    "Voices in my head\"\"\"\n",
    "pattern = r'^[A-Z]'\n",
    "pattern1 = r'\\?$'\n",
    "pattern2 = r'es$' # finds nothing because it thinks of the line as the whole string and no line ends with 'es', so in order it to work I have to tokenize it first \n",
    "\n",
    "nltk.re_show(pattern, paranoid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.143917Z",
     "start_time": "2024-04-04T10:08:42.141601Z"
    }
   },
   "id": "98a5c5477f6af1e",
   "execution_count": 128
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n",
      "as accurately as possible; as well as the; as faithfully as possible;\n",
      "as much as what; as neat as a; as simple as you; as well as other; as\n",
      "well as other; as involved as determining; as well as other; as\n",
      "important as another; as accurately as possible; as accurate as any;\n",
      "as much as any; as different as a; as Orphic as that; as coppery as\n",
      "Delawares; as good as another; as large as small; as well as ease; as\n",
      "well as their; as well as possible; as straight as possible; as well\n",
      "as nailed; as smoothly as the; as soon as a; as well as injuries; as\n",
      "well as many; as well as reason; as well as in; as well as of; as well\n",
      "as a; as well as summer; as well as providing; as important as\n",
      "cooling; as evenly as it; as much as shading; as well as some; as well\n",
      "as subsoil; as high as possible; as well as many; as general as\n",
      "electrical; as long as the; as well as the; as much as was; as well as\n",
      "set; as well as by; as high as 15; as well as aid; as much as\n",
      "possible; as well as personalities; as low as a; as well as the; as\n",
      "much as glass; as popular as renting; as expensive as most; as well as\n",
      "relative; as well as by; as well as the; as far as possible; as far as\n",
      "radiation; as well as theoretical; as well as nuclear; as small as\n",
      "possible; as well as soap; as effective as the; as much as\n",
      "approximately; as well as information; as little as one; as much as\n",
      "an; as low as Af; as long as the; as far as possible; as well as\n",
      "their; as well as Hand; as well as all; as well as fractionation; as\n",
      "potent as the; as well as fever; as large as 3; as well as varying; as\n",
      "well as the; as long as 2; as far as emotional; as well as the; as\n",
      "well as regarding; as well as enthusiasm; as well as by; as well as\n",
      "her; as well as a; as old as social; as well as the; as well as the;\n",
      "as well as in; as much as they; as much as possible; as well as the;\n",
      "as well as some; as simple as one; as well as the; as well as in; as\n",
      "definable as possible; as long as they; as well as their; as well as\n",
      "forecasting; as soon as possible; as inevitable as anything; as well\n",
      "as for; as well as for; as nebulous as the; as awkward as the; as well\n",
      "as the; as well as by; as well as those; as well as the; as well as\n",
      "an; as well as with; as well as the; as well as moral; as much as\n",
      "their; as well as that; as likely as not; as well as upon; as well as\n",
      "on; as well as upon; as long as all; as far as one; as long as the; as\n",
      "empty as the; as well as the; as well as the; as soon as they; as well\n",
      "as office; as speedily as possible; as well as of; as well as start;\n",
      "as well as behind; as much as for; as effectively as they; as\n",
      "important as it; as nearly as feasible; as well as form; as well as\n",
      "aesthetic; as well as ethical; as well as Impressionism; as well as\n",
      "the; as broad as the; as much as he; as arresting as a; as odd as the;\n",
      "as well as the; as soon as possible; as long as it; as impassive as\n",
      "Persian; as long as those; as importantly as his; as well as\n",
      "providing; as well as the; as well as vertically; as well as new; as\n",
      "well as certain; as well as the; as close as possible; as far as\n",
      "obtainable; as well as the; as important as the; as long as the; as\n",
      "satisfactory as those\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown \n",
    "hobbies_learned = nltk.Text(brown.words(categories = ['hobbies', 'learned']))\n",
    "hobbies_learned.findall(r'<\\w*> <and> <other> <\\w*s>')\n",
    "hobbies_learned.findall(r'<as> <\\w*> <as> <\\w*>')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.607397Z",
     "start_time": "2024-04-04T10:08:42.144845Z"
    }
   },
   "id": "2b4e98daacedbb63",
   "execution_count": 129
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercises \n",
    "#### 1. Define a string s = 'colorless'. Write a Python statement that changes this to \"colourless\" using only the slice and concatenation operations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95502eba563befe0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colourless\n"
     ]
    }
   ],
   "source": [
    "s = 'colorless' \n",
    "print(s[:4] + 'u' + s[4:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.610104Z",
     "start_time": "2024-04-04T10:08:42.608070Z"
    }
   },
   "id": "76a23c7b827e1613",
   "execution_count": 130
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality, un-do, pre-heat."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9281da907ef951ce"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish', 'run', 'nation', 'do', 'heat']\n"
     ]
    }
   ],
   "source": [
    "ex2_words = [w for w in 'dish-es, run-ning, nation-ality, un-do, pre-heat'.split(', ')]\n",
    "ex2_noaff = [ex2_words[0][:-3], ex2_words[1][:-5], ex2_words[2][:-6], ex2_words[3][-2:], ex2_words[4][-4:]]\n",
    "print(ex2_noaff)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.613406Z",
     "start_time": "2024-04-04T10:08:42.610701Z"
    }
   },
   "id": "3f1bab5ab0e0ad45",
   "execution_count": 131
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string? \n",
    "I don't think so. I cannot use negative numbers as they remove chars and the lowest possible is then 0 which is the first char.\n",
    " \n",
    "Okay, I see it now. I thought about slicing but not about indexing. So it's possible to raise the error then. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "232e2c1ec1fc3f3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "o\n",
      "h\n"
     ]
    }
   ],
   "source": [
    "s = \"hello\"\n",
    "print(s[-6:])  # This will print 'hello'\n",
    "print(s[-100:])  # Still prints 'hello', as slicing is more forgiving\n",
    "print(s[-1])  # This will print 'o'\n",
    "print(s[-5])  # This will print 'h'\n",
    "# Accessing beyond the start (too far to the left)\n",
    "#print(s[-6])  # This will raise an IndexError"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.616132Z",
     "start_time": "2024-04-04T10:08:42.614098Z"
    }
   },
   "id": "c1e07d4ad2481ab3",
   "execution_count": 132
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. We can specify a \"step\" size for the slice. The following returns every second character within the slice: monty[6:11:2]. It also works in the reverse direction: monty[10:5:-2] Try these for yourself, then experiment with different step values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fc4ba639f33690e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pes ol o tptenie\n",
      "' rn e oers\n",
      "rmalteubr hce\n",
      "ocsi yha\n",
      "lo \n"
     ]
    }
   ],
   "source": [
    "print(paranoid[::2])\n",
    "print(paranoid[10:5:-2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.618359Z",
     "start_time": "2024-04-04T10:08:42.616662Z"
    }
   },
   "id": "e5158f4a4b4754e1",
   "execution_count": 133
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. What happens if you ask the interpreter to evaluate monty[::-1]? Explain why this is a reasonable result.\n",
    "It reverses a string, but why can it be reasonable? Perhaps if some text is reversed, it can reverse it back. Or to print some text so it's readable in mirrors. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "832f6cb4e71a2fad"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daeh ym ni secioV\n",
      "nekcihc nrobnu eht lla morF\n",
      "tser emos teg anyrt m'I\n",
      "?esion eht pots uoy dluoc esaelP\n",
      "Please could you stop the noise?\n",
      "I'm tryna get some rest\n",
      "From all the unborn chicken\n",
      "Voices in my head\n"
     ]
    }
   ],
   "source": [
    "print(paranoid[::-1])\n",
    "print(paranoid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.620868Z",
     "start_time": "2024-04-04T10:08:42.619024Z"
    }
   },
   "id": "4d0c7290d472711",
   "execution_count": 134
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6. Describe the class of strings matched by the following regular expressions. \n",
    "1. ```[a-zA-Z]+```: '+' means one or more of previous item, in this case one or more of any letter, case doesn't matter ('cat', 'so') > any word \n",
    "2. ```[A-Z][a-z]*```: here case matters, so first it should be an upper one then zero or more of any lower case ('D', 'Dog') > any upper case word \n",
    "3. ```p[aeiou]{,2}t```: a sequence should start with 'p' and end with 't', then between there should be at most 2 vowels > too much output, replacing it with ```^p[aeiou]{,2}t$``` ('put', 'poet') \n",
    "4. ```\\d+(\\.\\d+)?```: one or more digits and zero or one ('?') of digits after the decimal point\n",
    "5. ```([^aeiou][aeiou][^aeiou])*```: zero or more instances of a chunk that doesn't start with a vowel then has one vowel and doen't end with the vowel, so its CVC* combinations (??)  -- huh? \n",
    "6. ```\\w+|[^\\w\\s]+```: any alphanumeric char OR something that doesn't start with any alphanumeric char and a white space -- anything that is not a space?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6166b84c0ec21f4e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Please} {could} {you} {stop} {the} {noise}?\n",
      "{I}'{m} {tryna} {get} {some} {rest}\n",
      "{From} {all} {the} {unborn} {chicken}\n",
      "{Voices} {in} {my} {head}\n",
      "{Please} could you stop the noise?\n",
      "{I}'m tryna get some rest\n",
      "{From} all the unborn chicken\n",
      "{Voices} in my head\n",
      "['pat', 'paut', 'peat', 'pet', 'piet', 'pit', 'poet', 'poot', 'pot', 'pout', 'put']\n",
      "[]\n",
      "{}P{}l{}e{}a{se }{}c{}o{}u{}l{}d{} {}y{}o{}u{} {}s{top}{} {}t{he }{}n{}o{}i{se?}{}\n",
      "{}I{}'{}m{} {}t{}r{}y{na get}{} {som}{}e{} {res}{}t{}\n",
      "{}F{rom al}{}l{} {}t{he }{}u{}n{bor}{}n{} {}c{hicken}{}\n",
      "{}V{}o{}i{ces in}{} {}m{}y{} {}h{}e{}a{}d{}\n",
      "{Please} {could} {you} {stop} {the} {noise}{?}\n",
      "{I}{'}{m} {tryna} {get} {some} {rest}\n",
      "{From} {all} {the} {unborn} {chicken}\n",
      "{Voices} {in} {my} {head}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'[a-zA-Z]+', paranoid)\n",
    "nltk.re_show(r'[A-Z][a-z]*', paranoid)\n",
    "print([w for w in word_list if re.search(r'^p[aeiou]{,2}t$', w)])\n",
    "print([w for w in word_list if re.search(r'\\d+(\\.\\d+)?', w)])\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', paranoid)\n",
    "nltk.re_show(r'\\w+|[^\\w\\s]+', paranoid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.779711Z",
     "start_time": "2024-04-04T10:08:42.621478Z"
    }
   },
   "id": "c2f2afbe41377ed2",
   "execution_count": 135
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7. Write regular expressions to match the following classes of strings:\n",
    "\n",
    "1. A single determiner (assume that a, an, and the are the only determiners).\n",
    "2. An arithmetic expression using integers, addition, and multiplication, such as 2*3+8."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf9abdfb7d784930"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please could you stop {the} noise?\n",
      "I'm tryna get some rest\n",
      "From all {the} unborn chicken\n",
      "Voices in my head\n",
      "What's that? (I may be paranoid, but not {an} android)\n",
      "What's that? (I may be paranoid, but not {an} android)\n",
      "When I am king\n",
      "You will be first against {the} wall\n",
      "With your opinion\n",
      "Which is of no consequence at all\n",
      "What's that? (I may be paranoid, but no android)\n",
      "What's that? (I may be paranoid, but no android)\n",
      "La-la-la-la-la-la\n",
      "La-la-la-la-la-la\n",
      "La-la-la-la-la-la\n",
      "La, la\n",
      "Ambition makes you look pretty ugly\n",
      "Kicking, squealing, Gucci little piggy\n",
      "La-la-la-la-la-la\n",
      "La-la-la-la-la-la\n",
      "La-la-la-la-la-la\n",
      "La-la-la\n",
      "You don't remember, you don't remember\n",
      "Why don't you remember my name?\n",
      "Off with his head, man, off with his head, man\n",
      "Why don't you remember my name?\n",
      "I guess he does\n",
      "Ah, oh, oh, oh\n",
      "Oh, oh, oh, oh\n",
      "Oh, oh\n",
      "Oh, oh\n",
      "Rain down, rain down\n",
      "Come on, rain down on me\n",
      "From {a} great height\n",
      "From {a} great height, height\n",
      "Rain down, rain down\n",
      "Come on, rain down on me\n",
      "From {a} great height\n",
      "From {a} great height, height\n",
      "that's it, sir, you're leaving\n",
      "The crackle of pigskin (rain down)\n",
      "(Come on rain down) {the} dust and {the} screaming\n",
      "The yuppies networking\n",
      "The panic, {the} vomit (from {a} great height)\n",
      "The panic, {the} vomit (from {a} great height)\n",
      "God loves his children\n",
      "God loves his children, yeah\n"
     ]
    }
   ],
   "source": [
    "paranoid_full_raw = open('paranoid_full.txt')\n",
    "paranoid_full = paranoid_full_raw.read()\n",
    "nltk.re_show(r'(\\ban?\\b)|(\\bthe\\b)', paranoid_full)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.783401Z",
     "start_time": "2024-04-04T10:08:42.780343Z"
    }
   },
   "id": "e31c146a846ca13",
   "execution_count": 136
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2+2+2+2+2}, {2+2}, {5*4}, {73}{-74+73}{-74}, {34}/{2}, {2*3+8}, banana, apple, DNA, {451}Fahrenheit\n",
      "{2+2+2+2+2}, {2+2}, {5*4}, {73-74+73-74}, {34/2}, {2*3+8}, banana, apple, DNA, {451}Fahrenheit\n"
     ]
    }
   ],
   "source": [
    "arithmetics_7b = \"2+2+2+2+2, 2+2, 5*4, 73-74+73-74, 34/2, 2*3+8, banana, apple, DNA, 451Fahrenheit\" # {451}Fahrenheit is picked in both cases\n",
    "\n",
    "nltk.re_show(r'-?\\d+(\\s*[\\+\\*]\\s*-?\\d+)*', arithmetics_7b) # ChatGPT\n",
    "nltk.re_show(r'([^\\sa-zA-Z,]*[\\+\\*]*)*[^\\s,a-zA-Z]', arithmetics_7b) # my version "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:42.786819Z",
     "start_time": "2024-04-04T10:08:42.784122Z"
    }
   },
   "id": "9d690a524a3d3102",
   "execution_count": 137
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8. Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use from urllib import request and then request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8226bcde9bc96898"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NLTK :: Natural Language Toolkit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NLTK\n",
      "\n",
      "\n",
      "\n",
      "Documentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NLTK Documentation\n",
      "\n",
      "API Reference\n",
      "Example Usage\n",
      "Module Index\n",
      "Wiki\n",
      "FAQ\n",
      "Open Issues\n",
      "NLTK on GitHub\n",
      "\n",
      "Installation\n",
      "\n",
      "Installing NLTK\n",
      "Installing NLTK Data\n",
      "\n",
      "More\n",
      "\n",
      "Release Notes\n",
      "Contributing to NLTK\n",
      "NLTK Team\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Natural Language ToolkitÂ¶\n",
      "NLTK is a leading platform for building Python programs to work with human language data.\r\n",
      "It provides easy-to-use interfaces to over 50 corpora and lexical\r\n",
      "resources such as WordNet,\r\n",
      "along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\r\n",
      "wrappers for industrial-strength NLP libraries,\r\n",
      "and an active discussion forum.\n",
      "Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation,\r\n",
      "NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\r\n",
      "NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\n",
      "NLTK has been called âa wonderful tool for teaching, and working in, computational linguistics using Python,â\r\n",
      "and âan amazing library to play with natural language.â\n",
      "Natural Language Processing with Python provides a practical\r\n",
      "introduction to programming for language processing.\r\n",
      "Written by the creators of NLTK, it guides the reader through the fundamentals\r\n",
      "of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\r\n",
      "and more.\r\n",
      "The online version of the book has been been updated for Python 3 and NLTK 3.\r\n",
      "(The original Python 2 version is still available at https://www.nltk.org/book_1ed.)\n",
      "\n",
      "Some simple things you can do with NLTKÂ¶\n",
      "Tokenize and tag some text:\n",
      ">>> import nltk\r\n",
      ">>> sentence = \"\"\"At eight o'clock on Thursday morning\r\n",
      "... Arthur didn't feel very good.\"\"\"\r\n",
      ">>> tokens = nltk.word_tokenize(sentence)\r\n",
      ">>> tokens\r\n",
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning',\r\n",
      "'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\r\n",
      ">>> tagged = nltk.pos_tag(tokens)\r\n",
      ">>> tagged[0:6]\r\n",
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'),\r\n",
      "('Thursday', 'NNP'), ('morning', 'NN')]\r\n",
      "\n",
      "\n",
      "Identify named entities:\n",
      ">>> entities = nltk.chunk.ne_chunk(tagged)\r\n",
      ">>> entities\r\n",
      "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'),\r\n",
      "           ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'),\r\n",
      "       Tree('PERSON', [('Arthur', 'NNP')]),\r\n",
      "           ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'),\r\n",
      "           ('very', 'RB'), ('good', 'JJ'), ('.', '.')])\r\n",
      "\n",
      "\n",
      "Display a parse tree:\n",
      ">>> from nltk.corpus import treebank\r\n",
      ">>> t = treebank.parsed_sents('wsj_0001.mrg')[0]\r\n",
      ">>> t.draw()\r\n",
      "\n",
      "\n",
      "\n",
      "NB. If you publish work that uses NLTK, please cite the NLTK book as\r\n",
      "follows:\n",
      "\n",
      "Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python.  OâReilly Media Inc.\n",
      "\n",
      "\n",
      "\n",
      "Next StepsÂ¶\n",
      "\n",
      "Sign up for release announcements\n",
      "Join in the discussion\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " source\n",
      "\n",
      "\n",
      "3.8.1\n",
      "\n",
      "\r\n",
      "                    Jan 02, 2023\r\n",
      "                \n",
      "\n",
      "\r\n",
      "                Â© 2023, NLTK Project\r\n",
      "            \n",
      "\r\n",
      "            created with Sphinx and NLTK Theme\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          PQs workshop (BCL 2024) | Maria Onoeva                      Maria Onoeva  Toggle navigation        about me   PQs workshop (BCL 2024)(current)   publications   projects   cv                     PQs workshop (BCL 2024) Our workshop at The Biennial of Czech Linguistics 2024   We are happy to announce that our workshop Polar (yes/no) questions: form, meaning and more has been accepted to The Biennial of Czech Linguistics. The Biennial 2024 is the first edition of a new conference taking place at the Faculty of Arts, Charles University on September 18-20, 2024. The concept of the conference builds on the model applied at the European level (the conference of Societas Linguistica Europaea) or in Germany (the conference of the Deutsche Gesellschaft fÃ¼r Sprachwissenschaft). Here you find more information about our workshop (CfP, general description, etc.) and the conference. In case of any questions contact us at mariia.razguliaeva.1@hu-berlin.de or onoevam@ff.cuni.cz. Workshop description This panel aims to bring together researchers who work on forms, meanings, and functions of polar questions (PQs) in their intra- and cross-linguistic variability. We invite a diverse range of submissions that draw upon theoretical observations, experimental results, corpus research, and other relevant perspectives. The goal is to establish a productive dialog between different approaches and gain new insights into the nature of PQs. It is planned to address the following research questions (the list is not exhaustive): 1) How do forms of PQs map to various facets of their meanings across languages? 2) Which contexts can be considered natural for certain PQ types? 3) How can the meaning of PQs be formalized and what predictions emerge from certain formalizations? 4) How are PQs processed by speakers, as well as during language acquisition and/or learning? Invited speaker    We are delighted to announce the invited speaker for our workshop - Tue Trinh, a senior researcher in the ERC project Speech Acts in Grammar and Discourse (SPAGAD) / Research Area 4 'Semantics & Pragmatics' at the Leibniz-Zentrum Allgemeine Sprachwissenschaft in Berlin. Tue works on many fascinating topics in syntax, semantics and pragmatics and, luckily for us, contributes greatly to the polar questions discussion. In some of the recent works, Tue explores a relation between NPIs and epistemic bias (Trinh 2023) or describes the differences between Vietnamese and English polar questions (Trinh 2024). The topic of the talk will be provided soon.              Call for papers Here is a summarized version of the call, for more visit the Biennial 2024 Call  Deadline: March 25, 2024 Platform: OpenReview (please register before submitting), the Biennial OpenReview page  Template: here  Our panel number: 16 One person can submit at most 3 abstracts to 2 workshops All abstracts must be anonymous and in the pdf format     Important dates Â      October 1, 2023: Deadline for submission of thematic workshop proposals   November 17, 2023: Notification of workshop acceptance   November 30, 2023: Call for abstracts to be issued   February 6, 2024: The abstract submission system has been launched.   March 25, 2024: Deadline for submission of abstracts   May 31, 2024: Final draft of the programme of workshops   June 30, 2024: Final draft programme of the Biennial   September 18-20, 2024: The Biennial of Czech Linguistics         Â© Copyright 2024 Maria Onoeva. Powered by Jekyll with al-folio theme. Hosted by GitHub Pages. Last updated: March 03, 2024.                      \n"
     ]
    }
   ],
   "source": [
    "def url_opener(link):\n",
    "    url = link \n",
    "    response = request.urlopen(url) \n",
    "    raw = response.read().decode('utf8')\n",
    "    ready_url = BeautifulSoup(raw, 'html.parser').get_text()\n",
    "    print(ready_url) \n",
    "    \n",
    "url_opener('http://nltk.org/')\n",
    "url_opener('https://mariaonoeva.github.io/Biennial2024/')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:43.602456Z",
     "start_time": "2024-04-04T10:08:42.787489Z"
    }
   },
   "id": "ca2947c45db263da",
   "execution_count": 138
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9.  Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "1. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag (?x)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "622cd08fea2550f1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load(my_text_raw): \n",
    "    my_text = open(my_text_raw)\n",
    "    my_text_read = my_text.read() \n",
    "    return my_text_read\n",
    "\n",
    "def punct_counter(my_text):\n",
    "    counter_list = []\n",
    "    punct = nltk.regexp_tokenize(load(my_text), r'[^\\w\\s\\d]')\n",
    "    for i in punct: \n",
    "        counter_list.append(punct.count(i))\n",
    "    ready_dict = dict(zip(punct, counter_list))\n",
    "    sorted_dict = dict(sorted(ready_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    return sorted_dict.items()\n",
    "\n",
    "punct_counter('paranoid_full.txt')\n",
    "eng_HP = punct_counter('/Users/maria.onoeva/Desktop/new_folder/HP_1.txt')\n",
    "ru_HP = punct_counter('/Users/maria.onoeva/Desktop/new_folder/HP_1ru.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:53.087970Z",
     "start_time": "2024-04-04T10:08:43.603582Z"
    }
   },
   "id": "5c80419b6130c6c7",
   "execution_count": 139
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation      Count\n",
      "-------------  -------\n",
      ".                 6136\n",
      ",                 5658\n",
      "\"                 4758\n",
      "'                 3141\n",
      "-                 1990\n",
      "?                  754\n",
      "!                  474\n",
      ";                  135\n",
      ":                   69\n",
      ")                   33\n",
      "(                   30\n",
      "*                    2\n",
      "~                    1\n",
      "\\                    1\n",
      "Punctuation      Count\n",
      "-------------  -------\n",
      ",                10190\n",
      ".                 6355\n",
      "â                 4598\n",
      "-                 1038\n",
      "â¦                  805\n",
      "?                  761\n",
      "!                  620\n",
      "Â«                  222\n",
      "Â»                  221\n",
      ":                  140\n",
      "*                  117\n",
      "(                   13\n",
      ")                   13\n",
      ";                    3\n",
      "â                    1\n",
      "â                    1\n",
      "â                    1\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate \n",
    "headers = ['Punctuation', 'Count']\n",
    "\n",
    "print(tabulate(eng_HP, headers=headers))\n",
    "print(tabulate(ru_HP, headers=headers))\n",
    "\n",
    "# I want them side by side \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:53.091782Z",
     "start_time": "2024-04-04T10:08:53.088873Z"
    }
   },
   "id": "3c311df5b5574fa9",
   "execution_count": 140
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Harry Potter 1 punctuation: Eng and Ru \n",
    "##### Eng: \n",
    "- why are there 30 of '(' and 33 of ')'? \n",
    "- questions: 754\n",
    "##### Ru: \n",
    "- same for Ru for kavychki, but skobki are even\n",
    "- so many commas??? \n",
    "- questions: 761 > 7 more > pretty consistent, I can imagine contexts where several questions mark used to express anger or surprise "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a7ead94457cbb77"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation ENG      Count    Punctuation RU      Count\n",
      ".                     6136    ,                   10190\n",
      ",                     5658    .                    6355\n",
      "\"                     4758    â                    4598\n",
      "'                     3141    -                    1038\n",
      "-                     1990    â¦                     805\n",
      "?                      754    ?                     761\n",
      "!                      474    !                     620\n",
      ";                      135    Â«                     222\n",
      ":                       69    Â»                     221\n",
      ")                       33    :                     140\n",
      "(                       30    *                     117\n",
      "*                        2    (                      13\n",
      "~                        1    )                      13\n",
      "\\                        1    ;                       3\n",
      "                              â                       1\n",
      "                              â                       1\n",
      "                              â                       1\n"
     ]
    }
   ],
   "source": [
    "# ChatGPT to display the tables side by side\n",
    "# Generate table strings\n",
    "table1 = tabulate(eng_HP, headers=['Punctuation ENG', 'Count'], tablefmt='plain')\n",
    "table2 = tabulate(ru_HP, headers=['Punctuation RU', 'Count'], tablefmt='plain')\n",
    "\n",
    "# Split tables into lines\n",
    "lines1 = table1.split('\\n')\n",
    "lines2 = table2.split('\\n')\n",
    "\n",
    "# Determine the max width of the first table\n",
    "max_width = max(len(line) for line in lines1)\n",
    "\n",
    "# Determine the longer table for iteration\n",
    "longest_table_length = max(len(lines1), len(lines2))\n",
    "\n",
    "# Print tables side by side, handling uneven number of rows\n",
    "for i in range(longest_table_length):\n",
    "    line1 = lines1[i] if i < len(lines1) else \"\"\n",
    "    line2 = lines2[i] if i < len(lines2) else \"\"\n",
    "    print(f\"{line1.ljust(max_width)}    {line2}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:53.096056Z",
     "start_time": "2024-04-04T10:08:53.092542Z"
    }
   },
   "id": "cc8336f6f19a8953",
   "execution_count": 141
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9. Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "2. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc0647d4a5dd8fff"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{200$}, {363524$}, {587$}, {65â¬}, {8746CZK}, {8â½}, {9382â½}, {87362513537CZK}, Grand Theft Auto V, Corpus paper, April, Thursday, 76F, 17C\n"
     ]
    }
   ],
   "source": [
    "# monetary amounts? \n",
    "monetary_amounts = \"200$, 363524$, 587$, 65â¬, 8746CZK, 8â½, 9382â½, 87362513537CZK, Grand Theft Auto V, Corpus paper, April, Thursday, 76F, 17C\"\n",
    "money_pattern = r'\\d+([\\$â¬â½]|CZK)+' \n",
    "\n",
    "nltk.re_show(money_pattern, monetary_amounts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:53.099065Z",
     "start_time": "2024-04-04T10:08:53.096952Z"
    }
   },
   "id": "6ca3e7d8db41177",
   "execution_count": 142
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2024-04-04}, {2024-04-05}, Your Top Songs 2023, {2024-04-06}, pinguin, {2024-04-07}, {2024-04-08}, Ice-cream, {2024-04-09}, {2024-04-10}, SnÄÅ¾ka\n",
      "2024-04-04, 2024-04-05, {Y}our {T}op {S}ongs 2023, 2024-04-06, pinguin, 2024-04-07, 2024-04-08, {I}ce-cream, 2024-04-09, 2024-04-10, {S}nÄÅ¾ka\n"
     ]
    }
   ],
   "source": [
    "# dates\n",
    "dates_more = '2024-04-04, 2024-04-05, Your Top Songs 2023, 2024-04-06, pinguin, 2024-04-07, 2024-04-08, Ice-cream, 2024-04-09, 2024-04-10, SnÄÅ¾ka'\n",
    "dates_pattern = r'\\d{4}-\\d{2}-\\d{2}'\n",
    "nltk.re_show(dates_pattern, dates_more)\n",
    "\n",
    "# names of people and organizations\n",
    "names_pattern = r'[A-Z]+'\n",
    "nltk.re_show(names_pattern, dates_more)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:10:57.034317Z",
     "start_time": "2024-04-04T10:10:57.025253Z"
    }
   },
   "id": "1b58f300e8e7699f",
   "execution_count": 146
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:08:53.109312Z",
     "start_time": "2024-04-04T10:08:53.107961Z"
    }
   },
   "id": "b882aba6c4d7b52a",
   "execution_count": 144
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
